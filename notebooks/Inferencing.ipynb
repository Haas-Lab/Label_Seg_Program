{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219919d9",
   "metadata": {},
   "source": [
    "## Inferencing Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f678db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonfung/miniforge3/envs/ml_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/jasonfung/miniforge3/envs/ml_env/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/jasonfung/miniforge3/envs/ml_env/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowExxb\n",
      "  Referenced from: /Users/jasonfung/miniforge3/envs/ml_env/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Expected in: /Users/jasonfung/miniforge3/envs/ml_env/lib/python3.9/site-packages/torch/lib/libc10.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "# import core libaries\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import tifffile\n",
    "\n",
    "SCRIPT_DIR = os.path.dirname(os.path.abspath(__vsc_ipynb_file__))\n",
    "sys.path.append(os.path.dirname(SCRIPT_DIR))\n",
    "from processing.processing_functions import *\n",
    "\n",
    "# get working directory\n",
    "path = os.getcwd()\n",
    "sys.path.append(path)\n",
    "\n",
    "# import machine learning libraries\n",
    "import torch\n",
    "from torchvision import transforms, utils\n",
    "from monai.inferers.inferer import SlidingWindowInferer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55de52bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize cuda if available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5eda705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"+s+d+f_ResUNet.onnx\"\n",
    "model_soma_dendrite = \"Soma+Dendrite.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e681a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing raw image\n",
    "lateral_steps = 64\n",
    "axial_steps = 16\n",
    "patch_size = (axial_steps, lateral_steps, lateral_steps)\n",
    "batch_size = 64\n",
    "# split_size = 0.9\n",
    "dim_order = (0,4,1,2,3) # define the image and mask dimension order\n",
    "\n",
    "raw_path = filedialog.askopenfilename()\n",
    "raw_img = glob.glob(raw_path)\n",
    "orig_shape = tifffile.imread(raw_img).shape\n",
    "\n",
    "# Use patch transform to normalize and transform ndarray(z,y,x) -> tensor(\n",
    "patch_transform = transforms.Compose([MinMaxScalerVectorized(),\n",
    "                                      patch_imgs(xy_step = lateral_steps, z_step = axial_steps, patch_size = patch_size, is_mask = False)])\n",
    "\n",
    "\n",
    "processed_test_img = MyImageDataset(raw_list = raw_img,\n",
    "                                    mask_list = None,\n",
    "                                    transform = patch_transform,\n",
    "                                    device = device,\n",
    "                                    img_order = dim_order,\n",
    "                                    mask_order = dim_order,\n",
    "                                    num_classes = None,\n",
    "                                    train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf87454",
   "metadata": {},
   "source": [
    "## Using Custom Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d62d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reconstructed_img = inference(processed_test_img, \n",
    "                              model, \n",
    "                              batch_size, \n",
    "                              patch_size, \n",
    "                              orig_shape,\n",
    "                              )\n",
    "\n",
    "np.unique(reconstructed_img)\n",
    "\n",
    "if len(np.unique(reconstructed_img))-1 == 2:\n",
    "    reconstructed_img[reconstructed_img==1] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9978bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(reconstructed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bcbe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tifffile.imwrite(f'{raw_path}_+s+d+f.tif', reconstructed_img.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e859310d",
   "metadata": {},
   "source": [
    "## Using MONAI Sliding Window Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f7aad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "onnx_model = onnx.load(f\"/Users/jasonfung/Documents/Label_Seg_Program/models/{model_soma_dendrite}\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4e5651e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lateral_steps = 64\n",
    "axial_steps = 16\n",
    "patch_size = (axial_steps, lateral_steps, lateral_steps)\n",
    "batch_size = 64\n",
    "\n",
    "inferer = SlidingWindowInferer(roi_size=patch_size, sw_batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec74e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick test image\n",
    "raw_path = filedialog.askopenfilename()\n",
    "raw_img = glob.glob(raw_path)\n",
    "patch_transform = transforms.Compose([MinMaxScalerVectorized()])\n",
    "\n",
    "processed_test_img = MyImageDataset(raw_list = raw_img,\n",
    "                                    mask_list = None,\n",
    "                                    transform = patch_transform,\n",
    "                                    device = device,\n",
    "                                    test_model = True,\n",
    "                                    train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a53884b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(processed_test_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cddef9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.unsqueeze(torch.unsqueeze(x,0),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0dca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62e429c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 35, 512, 512])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36b269db",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ModelProto' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jasonfung/Documents/Label_Seg_Program/notebooks/Inferencing.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jasonfung/Documents/Label_Seg_Program/notebooks/Inferencing.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jasonfung/Documents/Label_Seg_Program/notebooks/Inferencing.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     pred \u001b[39m=\u001b[39m inferer(inputs \u001b[39m=\u001b[39;49m x, network\u001b[39m=\u001b[39;49monnx_model)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.9/site-packages/monai/inferers/inferer.py:192\u001b[0m, in \u001b[0;36mSlidingWindowInferer.__call__\u001b[0;34m(self, inputs, network, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m    176\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    177\u001b[0m     inputs: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    181\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[torch\u001b[39m.\u001b[39mTensor, Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m], Dict[Any, torch\u001b[39m.\u001b[39mTensor]]:\n\u001b[1;32m    182\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \n\u001b[1;32m    184\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    190\u001b[0m \n\u001b[1;32m    191\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     \u001b[39mreturn\u001b[39;00m sliding_window_inference(  \u001b[39m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    193\u001b[0m         inputs,\n\u001b[1;32m    194\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroi_size,\n\u001b[1;32m    195\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msw_batch_size,\n\u001b[1;32m    196\u001b[0m         network,\n\u001b[1;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moverlap,\n\u001b[1;32m    198\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    199\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msigma_scale,\n\u001b[1;32m    200\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_mode,\n\u001b[1;32m    201\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcval,\n\u001b[1;32m    202\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msw_device,\n\u001b[1;32m    203\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice,\n\u001b[1;32m    204\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprogress,\n\u001b[1;32m    205\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroi_weight_map,\n\u001b[1;32m    206\u001b[0m         \u001b[39m*\u001b[39;49margs,\n\u001b[1;32m    207\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    208\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/ml_env/lib/python3.9/site-packages/monai/inferers/utils.py:176\u001b[0m, in \u001b[0;36msliding_window_inference\u001b[0;34m(inputs, roi_size, sw_batch_size, predictor, overlap, mode, sigma_scale, padding_mode, cval, sw_device, device, progress, roi_weight_map, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m unravel_slice \u001b[39m=\u001b[39m [\n\u001b[1;32m    172\u001b[0m     [\u001b[39mslice\u001b[39m(\u001b[39mint\u001b[39m(idx \u001b[39m/\u001b[39m num_win), \u001b[39mint\u001b[39m(idx \u001b[39m/\u001b[39m num_win) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m)] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(slices[idx \u001b[39m%\u001b[39m num_win])\n\u001b[1;32m    173\u001b[0m     \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m slice_range\n\u001b[1;32m    174\u001b[0m ]\n\u001b[1;32m    175\u001b[0m window_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([inputs[win_slice] \u001b[39mfor\u001b[39;00m win_slice \u001b[39min\u001b[39;00m unravel_slice])\u001b[39m.\u001b[39mto(sw_device)\n\u001b[0;32m--> 176\u001b[0m seg_prob_out \u001b[39m=\u001b[39m predictor(window_data, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# batched patch segmentation\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[39m# convert seg_prob_out to tuple seg_prob_tuple, this does not allocate new memory.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m seg_prob_tuple: Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ModelProto' object is not callable"
     ]
    }
   ],
   "source": [
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(onnx_model)\n",
    "inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
    "output = to_torch(ort_session.run(None,inputs)) # predict the batches\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = inferer(inputs = x, network=onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5953174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import napari\n",
    "# viewer = napari.Viewer()\n",
    "# orig_img = tifffile.imread(raw_img)\n",
    "# raw_image = viewer.add_image(orig_img, rgb=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577942a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_img = viewer.add_labels(reconstructed_img.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e794aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ml_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e948cd2eddc2b56aed0b51f92bfb3429aca2637a323db441b1bbdcb5065963e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
